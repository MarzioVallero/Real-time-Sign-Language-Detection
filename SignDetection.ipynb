{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-time Sign Language Detection\n",
    "\n",
    "Be sure to be in the root directory of the project before running the following code boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the required libraries\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "import numpy as np\n",
    "from numpy import sin, cos, pi, arctan\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Utility definitions\n",
    "WORKSPACE_PATH = 'Tensorflow/workspace'\n",
    "ANNOTATION_PATH = WORKSPACE_PATH+'/annotations'\n",
    "MODEL_PATH = WORKSPACE_PATH+'/models'\n",
    "PRETRAINED_MODEL_PATH = WORKSPACE_PATH+'/pre-trained-models'\n",
    "CONFIG_PATH = MODEL_PATH+'/my_ssd_mobnet/pipeline.config'\n",
    "CHECKPOINT_PATH = MODEL_PATH+'/my_ssd_mobnet/'\n",
    "SIN_LEFT_THETA = 2 * sin(pi / 4)\n",
    "SIN_UP_THETA = sin(pi / 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to change the argument of the \"ckpt.restore()\" function to the name of your resulting model's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pipeline.config file and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore the specified checkpoint (it must match an existing model)\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(CHECKPOINT_PATH, 'ckpt-41')).expect_partial()\n",
    "\n",
    "# Computes the detections from the predictive model\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Gestures in Real-Time\n",
    "\n",
    "The user should be located at a distance of 1 to 1.5 meters from the camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Externals.service.head_pose import HeadPoseEstimator\n",
    "from Externals.service.face_alignment import CoordinateAlignmentModel\n",
    "from Externals.service.face_detector import MxnetDetectionModel\n",
    "from Externals.service.iris_localization import IrisLocalizationModel\n",
    "import time\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "import sys\n",
    "import cv2 \n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import tkinter as tk\n",
    "import ast\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from PyQt5 import QtGui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_3d_gaze(frame, poi, scale=256):\n",
    "    starts, ends, pupils, centers = poi\n",
    "\n",
    "    eye_length = norm(starts - ends, axis=1)\n",
    "    ic_distance = norm(pupils - centers, axis=1)\n",
    "    zc_distance = norm(pupils - starts, axis=1)\n",
    "\n",
    "    s0 = (starts[:, 1] - ends[:, 1]) * pupils[:, 0]\n",
    "    s1 = (starts[:, 0] - ends[:, 0]) * pupils[:, 1]\n",
    "    s2 = starts[:, 0] * ends[:, 1]\n",
    "    s3 = starts[:, 1] * ends[:, 0]\n",
    "\n",
    "    delta_y = (s0 - s1 + s2 - s3) / eye_length / 2\n",
    "    delta_x = np.sqrt(abs(ic_distance**2 - delta_y**2))\n",
    "\n",
    "    delta = np.array((delta_x * SIN_LEFT_THETA,\n",
    "                      delta_y * SIN_UP_THETA))\n",
    "    delta /= eye_length\n",
    "    theta, pha = np.arcsin(delta)\n",
    "\n",
    "    # print(f\"THETA:{180 * theta / pi}, PHA:{180 * pha / pi}\")\n",
    "    # delta[0, abs(theta) < 0.1] = 0\n",
    "    # delta[1, abs(pha) < 0.03] = 0\n",
    "\n",
    "    inv_judge = zc_distance**2 - delta_y**2 < eye_length**2 / 4\n",
    "\n",
    "    delta[0, inv_judge] *= -1\n",
    "    theta[inv_judge] *= -1\n",
    "    delta *= scale\n",
    "\n",
    "    # cv2.circle(frame, tuple(pupil.astype(int)), 2, (0, 255, 255), -1)\n",
    "    # cv2.circle(frame, tuple(center.astype(int)), 1, (0, 0, 255), -1)\n",
    "\n",
    "    return theta, pha, delta.T\n",
    "\n",
    "def draw_sticker(src, offset, pupils, landmarks,\n",
    "                 blink_thd=0.22,\n",
    "                 arrow_color=(0, 125, 255), copy=False):\n",
    "    if copy:\n",
    "        src = src.copy()\n",
    "\n",
    "    left_eye_hight = landmarks[33, 1] - landmarks[40, 1]\n",
    "    left_eye_width = landmarks[39, 0] - landmarks[35, 0]\n",
    "\n",
    "    right_eye_hight = landmarks[87, 1] - landmarks[94, 1]\n",
    "    right_eye_width = landmarks[93, 0] - landmarks[89, 0]\n",
    "\n",
    "    for mark in landmarks.reshape(-1, 2).astype(int):\n",
    "        cv2.circle(src, tuple(mark), radius=1,\n",
    "                   color=(0, 0, 255), thickness=-1)\n",
    "\n",
    "    if left_eye_hight / left_eye_width > blink_thd:\n",
    "        cv2.arrowedLine(src, tuple(pupils[0].astype(int)),\n",
    "                        tuple((offset+pupils[0]).astype(int)), arrow_color, 2)\n",
    "\n",
    "    if right_eye_hight / right_eye_width > blink_thd:\n",
    "        cv2.arrowedLine(src, tuple(pupils[1].astype(int)),\n",
    "                        tuple((offset+pupils[1]).astype(int)), arrow_color, 2)\n",
    "\n",
    "    return src\n",
    "\n",
    "def handle_close(event, cap):\n",
    "    cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels from the label map\n",
    "category_index = label_map_util.create_category_index_from_labelmap(ANNOTATION_PATH+'/label_map.pbtxt')\n",
    "\n",
    "# Setup camera capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "root = tk.Tk()\n",
    "screen_width = root.winfo_screenwidth()\n",
    "screen_height = root.winfo_screenheight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration file found\n",
      "You aren't looking!\n",
      "You aren't looking!\n",
      "You aren't looking!\n",
      "You aren't looking!\n",
      "You aren't looking!\n",
      "You aren't looking!\n",
      "You aren't looking!\n",
      "You aren't looking!\n",
      "You aren't looking!\n",
      "You aren't looking!\n",
      "You aren't looking!\n",
      "You aren't looking!\n",
      "You aren't looking!\n",
      "You aren't looking!\n",
      "You aren't looking!\n",
      "You aren't looking!\n",
      "You aren't looking!\n",
      "You aren't looking!\n",
      "You aren't looking!\n",
      "You aren't looking!\n"
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "\n",
    "# Setup output window\n",
    "plt.ion()\n",
    "gui = plt.figure(\"Real-time Sign Detection\", facecolor='#1e1e1e', edgecolor='#1e1e1e')\n",
    "gui.canvas.mpl_connect(\"close_event\", lambda event: handle_close(event, cap))\n",
    "result = None\n",
    "title_obj = plt.title('Real-time Sign Detection')\n",
    "plt.setp(title_obj, color='#d4d4d4')         #set the color of title to white\n",
    "\n",
    "# Setup Gaze recognition models\n",
    "fd = MxnetDetectionModel(\"Externals/weights/16and32\", 0, .6, gpu=-1)\n",
    "fa = CoordinateAlignmentModel('Externals/weights/2d106det', 0, gpu=-1)\n",
    "gs = IrisLocalizationModel(\"Externals/weights/iris_landmark.tflite\")\n",
    "hp = HeadPoseEstimator(\"Externals/weights/object_points.npy\", cap.get(3), cap.get(4))\n",
    "\n",
    "# Check if a configuration file exists, else load a predefined value set\n",
    "if os.path.isfile('Config\\config.dat'):\n",
    "    print(\"Configuration file found\")\n",
    "    file = open(\"Config\\config.dat\", \"r\")\n",
    "    contents = file.read()\n",
    "    config = ast.literal_eval(contents)\n",
    "    file.close()\n",
    "else:\n",
    "    print (\"Configuration file not found. Run CreateHSVProfile.py to create a local profile\")\n",
    "    config = {'HL': 0, 'SL': 29, 'VL': 24, 'HH': 40, 'SH': 255, 'VH': 255}\n",
    "\n",
    "# Camera loop\n",
    "while cap.isOpened(): \n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    looking = 0\n",
    "    bboxes = fd.detect(frame)\n",
    "\n",
    "    for landmarks in fa.get_landmarks(frame, bboxes, calibrate=True):\n",
    "        # calculate head pose\n",
    "        _, euler_angle = hp.get_head_pose(landmarks)\n",
    "        pitch, yaw, roll = euler_angle[:, 0]\n",
    "\n",
    "        eye_markers = np.take(landmarks, fa.eye_bound, axis=0)\n",
    "        \n",
    "        eye_centers = np.average(eye_markers, axis=1)\n",
    "        # eye_centers = landmarks[[34, 88]]\n",
    "        \n",
    "        # eye_lengths = np.linalg.norm(landmarks[[39, 93]] - landmarks[[35, 89]], axis=1)\n",
    "        eye_lengths = (landmarks[[39, 93]] - landmarks[[35, 89]])[:, 0]\n",
    "\n",
    "        iris_left = gs.get_mesh(frame, eye_lengths[0], eye_centers[0])\n",
    "        pupil_left, radius_left = gs.draw_pupil(iris_left, frame, thickness=0)\n",
    "\n",
    "        iris_right = gs.get_mesh(frame, eye_lengths[1], eye_centers[1])\n",
    "        pupil_right, radius_right = gs.draw_pupil(iris_right, frame, thickness=0)\n",
    "\n",
    "        pupils = np.array([pupil_left, pupil_right])\n",
    "\n",
    "        poi = landmarks[[35, 89]], landmarks[[39, 93]], pupils, eye_centers\n",
    "        theta, pha, delta = calculate_3d_gaze(frame, poi)\n",
    "\n",
    "        if yaw > 30:\n",
    "            end_mean = delta[0]\n",
    "        elif yaw < -30:\n",
    "            end_mean = delta[1]\n",
    "        else:\n",
    "            end_mean = np.average(delta, axis=0)\n",
    "\n",
    "        if end_mean[0] < 0:\n",
    "            zeta = arctan(end_mean[1] / end_mean[0]) + pi\n",
    "        else:\n",
    "            zeta = arctan(end_mean[1] / (end_mean[0] + 1e-7))\n",
    "\n",
    "        # print(zeta * 180 / pi)\n",
    "        # print(zeta)\n",
    "        if roll < 0:\n",
    "            roll += 180\n",
    "        else:\n",
    "            roll -= 180\n",
    "\n",
    "        real_angle = zeta + roll * pi / 180\n",
    "        # real_angle = zeta\n",
    "\n",
    "        # print(\"end mean:\", end_mean)\n",
    "        # print(roll, real_angle * 180 / pi)\n",
    "\n",
    "        R = norm(end_mean)\n",
    "        offset = R * cos(real_angle), R * sin(real_angle)\n",
    "\n",
    "        landmarks[[38, 92]] = landmarks[[34, 88]] = eye_centers\n",
    "\n",
    "        if(debug):\n",
    "            gs.draw_eye_markers(eye_markers, frame, thickness=1)\n",
    "            draw_sticker(frame, offset, pupils, landmarks)\n",
    "            cv2.circle(frame, tuple(pupil_right), radius_right, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.circle(frame, tuple(pupil_left), radius_left, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        if (R > 45):\n",
    "            print(\"You aren't looking!\")\n",
    "            looking = 5\n",
    "        elif (looking > 0):\n",
    "            looking -= 1\n",
    "\n",
    "    image_np = np.array(frame)\n",
    "    \n",
    "    if (looking == 0):\n",
    "        # Skin tone segmentation\n",
    "        # The frame is converted to HSV, then thresholded according to the Hue value\n",
    "        # according to the paper: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.718.1964&rep=rep1&type=pdf\n",
    "        HSV_Frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        totalMask = cv2.inRange(HSV_Frame, (config[\"HL\"], config[\"SL\"], config[\"VL\"]), (config[\"HH\"], config[\"SH\"], config[\"VH\"]))\n",
    "        totalMask = totalMask.astype(np.uint8)\n",
    "        \n",
    "        # Face removal, in order to give less room for error to the gesture classifier\n",
    "        # A Haar classifier detects the face, then adds its filled bounding box to the mask\n",
    "        haar_face = cv2.CascadeClassifier()\n",
    "        haar_face.load(cv2.samples.findFile(\"Externals/HaarFrontalFaceAlt.xml\"))\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray_frame = cv2.equalizeHist(gray_frame)\n",
    "        faces = haar_face.detectMultiScale(gray_frame, minSize=(int(0.2*height), int(0.2*height)))\n",
    "        for (x, y, w, h) in faces:\n",
    "            vertices = np.array([[x,y-int(0.3*h)], [x+w, y-int(0.3*h)], [x+w, y+h], [x, y+h]])\n",
    "            cv2.fillPoly(totalMask, pts = [vertices], color =(0,0,0))\n",
    "            if(debug):\n",
    "                cv2.rectangle(image_np, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        \n",
    "        # The mask finally undergoes the Opening operator in order to remove pepper noise,\n",
    "        # then gets applied as a bitwise operator to the frame\n",
    "        totalMask = cv2.morphologyEx(totalMask, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(7, 7)))\n",
    "        output = cv2.bitwise_and(frame, frame, mask = totalMask)\n",
    "            \n",
    "        # The masked image is then converted to a tensor for object detection\n",
    "        input_tensor = tf.convert_to_tensor(np.expand_dims(output, 0), dtype=tf.float32)\n",
    "        detections = detect_fn(input_tensor)\n",
    "        num_detections = int(detections.pop('num_detections'))\n",
    "        detections = {key: value[0, :num_detections].numpy()\n",
    "                    for key, value in detections.items()}\n",
    "        detections['num_qdetections'] = num_detections\n",
    "        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "        label_id_offset = 1\n",
    "\n",
    "        # The bounding boxes of all detected gestures are drawn on top of the original frame\n",
    "        # with their corresponding label.\n",
    "        # max_boxes_to_draw=1 doesn't let two overlapping gestures to be recognized at once\n",
    "        # min_score_thresh=.7 ignores all detections with an accuracy rate lower than 70%\n",
    "        viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                    image_np,\n",
    "                    detections['detection_boxes'],\n",
    "                    detections['detection_classes']+label_id_offset,\n",
    "                    detections['detection_scores'],\n",
    "                    category_index,\n",
    "                    use_normalized_coordinates=True,\n",
    "                    max_boxes_to_draw=1,\n",
    "                    min_score_thresh=.7,\n",
    "                    agnostic_mode=False)\n",
    "\n",
    "    # The output is displayed on an interactive window\n",
    "    #cv2.imshow('object detection',  image_np)\n",
    "    #cv2.imshow('Masked image', output)\n",
    "\n",
    "    image_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n",
    "    if result is None:\n",
    "        plt.axis(\"off\")\n",
    "        result = plt.imshow(image_np)\n",
    "        plt.title(\"Real-time Sign Detection\")\n",
    "        plt.show() \n",
    "    else:\n",
    "        result.set_data(image_np)\n",
    "        gui.canvas.draw()\n",
    "        gui.canvas.flush_events()\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        plt.close('all')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional code for skin tone thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rule_A(BGR_Frame):\n",
    "    B_Frame, G_Frame, R_Frame =  [BGR_Frame[...,BGR] for BGR in range(3)]# [...] is the same as [:,:]\n",
    "    #you can use the split built-in method in cv2 library to get the b,g,r components\n",
    "    #B_Frame, G_Frame, R_Frame  = cv2.split(BGR_Frame)\n",
    "    #i am using reduce built in method to get the maximum of a 3 given matrices\n",
    "    BRG_Max = np.maximum.reduce([B_Frame, G_Frame, R_Frame])\n",
    "    BRG_Min = np.minimum.reduce([B_Frame, G_Frame, R_Frame])\n",
    "    #at uniform daylight, The skin colour illumination's rule is defined by the following equation :\n",
    "    Rule_1 = np.logical_and.reduce([R_Frame > 95, G_Frame > 40, B_Frame > 20 ,\n",
    "                                 BRG_Max - BRG_Min > 15,abs(R_Frame - G_Frame) > 15, \n",
    "                                 R_Frame > G_Frame, R_Frame > B_Frame])\n",
    "    #the skin colour under flashlight or daylight lateral illumination rule is defined by the following equation :\n",
    "    Rule_2 = np.logical_and.reduce([R_Frame > 220, G_Frame > 210, B_Frame > 170,\n",
    "                         abs(R_Frame - G_Frame) <= 15, R_Frame > B_Frame, G_Frame > B_Frame])\n",
    "    #Rule_1 U Rule_2\n",
    "    RGB_Rule = np.logical_or(Rule_1, Rule_2)\n",
    "    #return the RGB mask\n",
    "    return RGB_Rule\n",
    "def lines(axis):\n",
    "    #equation(3)\n",
    "    line1 = 1.5862  * axis + 20\n",
    "    #equation(4)\n",
    "    line2 = 0.3448  * axis + 76.2069\n",
    "    #equation(5)\n",
    "    #the slope of this equation is not correct Cr ≥ -4.5652 × Cb + 234.5652\n",
    "    #it should be around -1  \n",
    "    line3 = -1.005 * axis + 234.5652\n",
    "    #equation(6)\n",
    "    line4 = -1.15   * axis + 301.75\n",
    "    #equation(7)\n",
    "    line5 = -2.2857 * axis + 432.85\n",
    "    return [line1,line2,line3,line4,line5]\n",
    "    #The five bounding rules of Cr-Cb \n",
    "def Rule_B(YCrCb_Frame):\n",
    "    Y_Frame,Cr_Frame, Cb_Frame = [YCrCb_Frame[...,YCrCb] for YCrCb in range(3)]\n",
    "    line1,line2,line3,line4,line5 = lines(Cb_Frame)\n",
    "    YCrCb_Rule = np.logical_and.reduce([line1 - Cr_Frame >= 0,\n",
    "                                        line2 - Cr_Frame <= 0,\n",
    "                                        line3 - Cr_Frame <= 0,\n",
    "                                        line4 - Cr_Frame >= 0,\n",
    "                                        line5 - Cr_Frame >= 0])\n",
    "    return YCrCb_Rule\n",
    "def Rule_C(HSV_Frame):\n",
    "    Hue,Sat,Val = [HSV_Frame[...,i] for i in range(3)]\n",
    "    #Change values depending on experimental observation\n",
    "    HSV_ = np.logical_or(Hue < 35, Hue > 140)\n",
    "    return HSV_\n",
    "\n",
    "    Ycbcr_Frame = cv2.cvtColor(output, cv2.COLOR_BGR2YCrCb)\n",
    "    skin_ = np.logical_and.reduce([Rule_A(output), Rule_B(Ycbcr_Frame), Rule_C(HSV_Frame)])\n",
    "    skin_frame = np.asarray(skin_, dtype=np.uint8)\n",
    "    contours, hierarchy = cv2.findContours(skin_frame, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cv2.drawContours(frame, contours, -1, (0, 255, 0), 1)\n",
    "    rects = []\n",
    "    for c in contours:\n",
    "        # get the bounding rect\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        # draw a green rectangle to visualize the bounding rect\n",
    "        if (w > 40 and h > 40) and (w < 300 and h < 300):\n",
    "            #pinhole distance\n",
    "            Distance1 = 11.5*(frame.shape[1]/float(w))\n",
    "            #camera distance\n",
    "            Distance2 = 15.0*((frame.shape[1] + 226.8)/float(w))\n",
    "            #print(\"\\npinhole distance = {:.2f} cm\\ncamera distance = {:.2f} cm\".format(Distance1,Distance2))\n",
    "            #print(\"Width = {} \\t Height = {}\".format(w,h))\n",
    "            rects.append(np.asarray([x,y,w,w*1.25], dtype=np.uint16))\n",
    "\n",
    "    for i,r in enumerate(rects):\n",
    "        x0,y0,w,h = r\n",
    "        cv2.rectangle(frame, (x0,y0),(x0+w,y0+h),(0,255,0),1)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
