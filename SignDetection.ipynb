{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-time Sign Language Detection\n",
    "\n",
    "Be sure to be in the root directory of the project before running the following code boxes.\n",
    "\n",
    "The script recognizes all the letters in the standard Italian alphabet, plus four special control gestures which are registered as specific couples of characters detected in quick succession:\n",
    "- space: Do an \"A\" gesture followed by an \"S\" gesture. Simply quicky move your thumb out from the A position. Appends a space to the active string buffer.\n",
    "- delete: Do a \"V\" gesture followed by a \"U\" gesture. Similar to the metaphorical action of \"cutting\". Deletes the last character in the active string buffer.\n",
    "- clear: Do an \"E\" gesture followed by an \"A\" gesture. Similar to the action of crushing something with your hand. Deletes all the characters in the active string buffer.\n",
    "- enter: Do a \"D\" gesture followed by an \"O\" gesture. Similar to the action of clicking a camera's shutter button. Prints the active string buffer to stdout, then deletes all the characters in the active string buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Trained Model\n",
    "\n",
    "Here you can also change the tuples of consecutive characters associated to a special gesture, if you want to customize them. Beware that you have to switch between them in the time of 1 frame, so very few tuples work well enough for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the required libraries\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "import numpy as np\n",
    "from numpy import sin, cos, pi, arctan\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Utility definitions\n",
    "WORKSPACE_PATH = 'Tensorflow/workspace'\n",
    "ANNOTATION_PATH = WORKSPACE_PATH+'/annotations'\n",
    "MODEL_PATH = WORKSPACE_PATH+'/models'\n",
    "PRETRAINED_MODEL_PATH = WORKSPACE_PATH+'/pre-trained-models'\n",
    "CONFIG_PATH = MODEL_PATH+'/my_ssd_mobnet/pipeline.config'\n",
    "CHECKPOINT_PATH = MODEL_PATH+'/my_ssd_mobnet/'\n",
    "SIN_LEFT_THETA = 2 * sin(pi / 4)\n",
    "SIN_UP_THETA = sin(pi / 6)\n",
    "specialGestures = {\"space\" : (\"A\", \"S\"), \"delete\" : (\"V\", \"U\"), \"clear\" : (\"E\", \"A\"), \"enter\" : (\"D\", \"O\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to change the argument of the \"ckpt.restore()\" function to the name of your resulting model's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pipeline.config file and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore the specified checkpoint (it must match an existing model)\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(CHECKPOINT_PATH, 'ckpt-41')).expect_partial()\n",
    "\n",
    "# Computes the detections from the predictive model\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Gestures in Real-Time\n",
    "\n",
    "The user should be located at a distance of 1 to 1.5 meters from the camera, else the model won't be able to recognize gestures with high enough accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Externals.service.head_pose import HeadPoseEstimator\n",
    "from Externals.service.face_alignment import CoordinateAlignmentModel\n",
    "from Externals.service.face_detector import MxnetDetectionModel\n",
    "from Externals.service.iris_localization import IrisLocalizationModel\n",
    "import time\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "import sys\n",
    "import cv2 \n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import tkinter as tk\n",
    "import ast\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from PyQt5 import QtGui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The draw_sticker function is only called in --Debug mode, so in order to see it in action, change the debug variable to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "\n",
    "def calculate_3d_gaze(frame, poi, scale=256):\n",
    "    starts, ends, pupils, centers = poi\n",
    "\n",
    "    eye_length = norm(starts - ends, axis=1)\n",
    "    ic_distance = norm(pupils - centers, axis=1)\n",
    "    zc_distance = norm(pupils - starts, axis=1)\n",
    "\n",
    "    s0 = (starts[:, 1] - ends[:, 1]) * pupils[:, 0]\n",
    "    s1 = (starts[:, 0] - ends[:, 0]) * pupils[:, 1]\n",
    "    s2 = starts[:, 0] * ends[:, 1]\n",
    "    s3 = starts[:, 1] * ends[:, 0]\n",
    "\n",
    "    delta_y = (s0 - s1 + s2 - s3) / eye_length / 2\n",
    "    delta_x = np.sqrt(abs(ic_distance**2 - delta_y**2))\n",
    "\n",
    "    delta = np.array((delta_x * SIN_LEFT_THETA,\n",
    "                      delta_y * SIN_UP_THETA))\n",
    "    delta /= eye_length\n",
    "    theta, pha = np.arcsin(delta)\n",
    "\n",
    "    # print(f\"THETA:{180 * theta / pi}, PHA:{180 * pha / pi}\")\n",
    "    # delta[0, abs(theta) < 0.1] = 0\n",
    "    # delta[1, abs(pha) < 0.03] = 0\n",
    "\n",
    "    inv_judge = zc_distance**2 - delta_y**2 < eye_length**2 / 4\n",
    "\n",
    "    delta[0, inv_judge] *= -1\n",
    "    theta[inv_judge] *= -1\n",
    "    delta *= scale\n",
    "\n",
    "    # cv2.circle(frame, tuple(pupil.astype(int)), 2, (0, 255, 255), -1)\n",
    "    # cv2.circle(frame, tuple(center.astype(int)), 1, (0, 0, 255), -1)\n",
    "\n",
    "    return theta, pha, delta.T\n",
    "\n",
    "def draw_sticker(src, offset, pupils, landmarks,\n",
    "                 blink_thd=0.22,\n",
    "                 arrow_color=(0, 125, 255), copy=False):\n",
    "    if copy:\n",
    "        src = src.copy()\n",
    "\n",
    "    left_eye_hight = landmarks[33, 1] - landmarks[40, 1]\n",
    "    left_eye_width = landmarks[39, 0] - landmarks[35, 0]\n",
    "\n",
    "    right_eye_hight = landmarks[87, 1] - landmarks[94, 1]\n",
    "    right_eye_width = landmarks[93, 0] - landmarks[89, 0]\n",
    "\n",
    "    for mark in landmarks.reshape(-1, 2).astype(int):\n",
    "        cv2.circle(src, tuple(mark), radius=1,\n",
    "                   color=(0, 0, 255), thickness=-1)\n",
    "\n",
    "    if left_eye_hight / left_eye_width > blink_thd:\n",
    "        cv2.arrowedLine(src, tuple(pupils[0].astype(int)),\n",
    "                        tuple((offset+pupils[0]).astype(int)), arrow_color, 2)\n",
    "\n",
    "    if right_eye_hight / right_eye_width > blink_thd:\n",
    "        cv2.arrowedLine(src, tuple(pupils[1].astype(int)),\n",
    "                        tuple((offset+pupils[1]).astype(int)), arrow_color, 2)\n",
    "\n",
    "    return src\n",
    "\n",
    "def handle_close(event, cap):\n",
    "    cap.release()\n",
    "\n",
    "# Keyboard interrupt handler\n",
    "def on_press(event):\n",
    "    if event.key == 'q':\n",
    "        print(\"You pressed \" + event.key + \", the program exited\")\n",
    "        cap.release()\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels from the label map\n",
    "category_index = label_map_util.create_category_index_from_labelmap(ANNOTATION_PATH+'/label_map.pbtxt')\n",
    "\n",
    "# Setup camera capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "root = tk.Tk()\n",
    "screen_width = root.winfo_screenwidth()\n",
    "screen_height = root.winfo_screenheight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The window is setup through the matplotlib library. If you prefer to use another library, simply grab the processed image at the end of the Camera Loop and use it wherever you want.  \n",
    "The gaze recognition models are then loaded.\n",
    "This code box also checks for the existence of a config.dat file to be used for HSV thresholding and either loads it in or generates a temporary dictionary with statistically relevant default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration file found\n"
     ]
    }
   ],
   "source": [
    "# Setup output window\n",
    "plt.ion()\n",
    "gui = plt.figure(\"Real-time Sign Detection\", facecolor='#1e1e1e', edgecolor='#1e1e1e')\n",
    "gui.canvas.mpl_connect(\"close_event\", lambda event: handle_close(event, cap))\n",
    "gui.canvas.mpl_connect('key_press_event', on_press)\n",
    "result = None\n",
    "title_obj = plt.title('Real-time Sign Detection')\n",
    "plt.setp(title_obj, color='#d4d4d4')         #set the color of title to white\n",
    "\n",
    "# Setup Gaze recognition models\n",
    "fd = MxnetDetectionModel(\"Externals/weights/16and32\", 0, .6, gpu=-1)\n",
    "fa = CoordinateAlignmentModel('Externals/weights/2d106det', 0, gpu=-1)\n",
    "gs = IrisLocalizationModel(\"Externals/weights/iris_landmark.tflite\")\n",
    "hp = HeadPoseEstimator(\"Externals/weights/object_points.npy\", cap.get(3), cap.get(4))\n",
    "\n",
    "# input buffers\n",
    "currentLetter = (\"\", 0)\n",
    "previousLetter = \"\"\n",
    "buffer = \"\"\n",
    "text_kwargs = dict(size=20, ha=\"center\", va=\"baseline\", bbox=dict(boxstyle=\"round\", ec=('#d4d4d4'), fc=('#1e1e1e')), color=('#d4d4d4'))\n",
    "text = plt.text(int(width*.5), height, buffer, **text_kwargs)\n",
    "\n",
    "# Check if a configuration file exists, else load a predefined value set\n",
    "if os.path.isfile('Config\\config.dat'):\n",
    "    print(\"Configuration file found\")\n",
    "    file = open(\"Config\\config.dat\", \"r\")\n",
    "    contents = file.read()\n",
    "    config = ast.literal_eval(contents)\n",
    "    file.close()\n",
    "else:\n",
    "    print (\"Configuration file not found. Run CreateHSVProfile.py to create a local profile\")\n",
    "    config = {'HL': 0, 'SL': 29, 'VL': 24, 'HH': 40, 'SH': 255, 'VH': 255}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main camera loop, can be dissected in four main parts:  \n",
    "- Gaze recognition: this step detects facial landmarks and eye position, estimating eye gaze direction through two 2D vectors to be applied on the output frame, on for each eye. The norm of the vector is then used as a parameter to determine the offset of the user's gaze direction with respect to the center. Empirical results proved that the value of R = 45 gave the best results.  \n",
    "- Sign recognition: this step is bound by the fact that the user is looking towards the screen. The frame is processed by another neural network, which detects signs and surrounds them with bounding boxes.\n",
    "- Input processing: this step processes detected gestures on a frame-by-frame basis. If a gesture is kept stable for five frames, it's added to the output string buffer. The special gestures are registered only if witching between one sign and the other occours in less than 5 frames.  \n",
    "- Display on output: this step takes the processed frames and displays them on output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You aren't looking!\n",
      "You aren't looking!\n",
      "You aren't looking!\n",
      "HELLO\n",
      "You aren't looking!\n",
      "You aren't looking!\n",
      "You pressed q, the program exited\n"
     ]
    }
   ],
   "source": [
    "# Camera loop\n",
    "while cap.isOpened(): \n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    looking = 0\n",
    "    bboxes = fd.detect(frame)\n",
    "\n",
    "    for landmarks in fa.get_landmarks(frame, bboxes, calibrate=True):\n",
    "        # calculate head pose\n",
    "        _, euler_angle = hp.get_head_pose(landmarks)\n",
    "        pitch, yaw, roll = euler_angle[:, 0]\n",
    "\n",
    "        eye_markers = np.take(landmarks, fa.eye_bound, axis=0)\n",
    "        \n",
    "        eye_centers = np.average(eye_markers, axis=1)\n",
    "        # eye_centers = landmarks[[34, 88]]\n",
    "        \n",
    "        # eye_lengths = np.linalg.norm(landmarks[[39, 93]] - landmarks[[35, 89]], axis=1)\n",
    "        eye_lengths = (landmarks[[39, 93]] - landmarks[[35, 89]])[:, 0]\n",
    "\n",
    "        iris_left = gs.get_mesh(frame, eye_lengths[0], eye_centers[0])\n",
    "        pupil_left, radius_left = gs.draw_pupil(iris_left, frame, thickness=0)\n",
    "\n",
    "        iris_right = gs.get_mesh(frame, eye_lengths[1], eye_centers[1])\n",
    "        pupil_right, radius_right = gs.draw_pupil(iris_right, frame, thickness=0)\n",
    "\n",
    "        pupils = np.array([pupil_left, pupil_right])\n",
    "\n",
    "        poi = landmarks[[35, 89]], landmarks[[39, 93]], pupils, eye_centers\n",
    "        theta, pha, delta = calculate_3d_gaze(frame, poi)\n",
    "\n",
    "        if yaw > 30:\n",
    "            end_mean = delta[0]\n",
    "        elif yaw < -30:\n",
    "            end_mean = delta[1]\n",
    "        else:\n",
    "            end_mean = np.average(delta, axis=0)\n",
    "\n",
    "        if end_mean[0] < 0:\n",
    "            zeta = arctan(end_mean[1] / end_mean[0]) + pi\n",
    "        else:\n",
    "            zeta = arctan(end_mean[1] / (end_mean[0] + 1e-7))\n",
    "\n",
    "        # print(zeta * 180 / pi)\n",
    "        # print(zeta)\n",
    "        if roll < 0:\n",
    "            roll += 180\n",
    "        else:\n",
    "            roll -= 180\n",
    "\n",
    "        real_angle = zeta + roll * pi / 180\n",
    "        # real_angle = zeta\n",
    "\n",
    "        # print(\"end mean:\", end_mean)\n",
    "        # print(roll, real_angle * 180 / pi)\n",
    "\n",
    "        R = norm(end_mean)\n",
    "        offset = R * cos(real_angle), R * sin(real_angle)\n",
    "\n",
    "        landmarks[[38, 92]] = landmarks[[34, 88]] = eye_centers\n",
    "\n",
    "        if(debug):\n",
    "            gs.draw_eye_markers(eye_markers, frame, thickness=1)\n",
    "            draw_sticker(frame, offset, pupils, landmarks)\n",
    "            cv2.circle(frame, tuple(pupil_right), radius_right, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.circle(frame, tuple(pupil_left), radius_left, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        if (R > 45):\n",
    "            print(\"You aren't looking!\")\n",
    "            looking = 5\n",
    "        elif (looking > 0):\n",
    "            looking -= 1\n",
    "\n",
    "    image_np = np.array(frame)\n",
    "    \n",
    "    if (looking == 0):\n",
    "        # Skin tone segmentation\n",
    "        # The frame is converted to HSV, then thresholded according to the Hue value\n",
    "        # according to the paper: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.718.1964&rep=rep1&type=pdf\n",
    "        HSV_Frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        totalMask = cv2.inRange(HSV_Frame, (config[\"HL\"], config[\"SL\"], config[\"VL\"]), (config[\"HH\"], config[\"SH\"], config[\"VH\"]))\n",
    "        totalMask = totalMask.astype(np.uint8)\n",
    "        \n",
    "        # Face removal, in order to give less room for error to the gesture classifier\n",
    "        # A Haar classifier detects the face, then adds its filled bounding box to the mask\n",
    "        haar_face = cv2.CascadeClassifier()\n",
    "        haar_face.load(cv2.samples.findFile(\"Externals/HaarFrontalFaceAlt.xml\"))\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray_frame = cv2.equalizeHist(gray_frame)\n",
    "        faces = haar_face.detectMultiScale(gray_frame, minSize=(int(0.2*height), int(0.2*height)))\n",
    "        for (x, y, w, h) in faces:\n",
    "            vertices = np.array([[x,y-int(0.3*h)], [x+w, y-int(0.3*h)], [x+w, y+h], [x, y+h]])\n",
    "            cv2.fillPoly(totalMask, pts = [vertices], color =(0,0,0))\n",
    "            if(debug):\n",
    "                cv2.rectangle(image_np, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        \n",
    "        # The mask finally undergoes the Opening operator in order to remove pepper noise,\n",
    "        # then gets applied as a bitwise operator to the frame\n",
    "        totalMask = cv2.morphologyEx(totalMask, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(7, 7)))\n",
    "        output = cv2.bitwise_and(frame, frame, mask = totalMask)\n",
    "            \n",
    "        # The masked image is then converted to a tensor for object detection\n",
    "        # The detections dictionary is formatted according to the objectdetectionAPI\n",
    "        input_tensor = tf.convert_to_tensor(np.expand_dims(output, 0), dtype=tf.float32)\n",
    "        detections = detect_fn(input_tensor)\n",
    "        num_detections = int(detections.pop('num_detections'))\n",
    "        detections = {key: value[0, :num_detections].numpy()\n",
    "                    for key, value in detections.items()}\n",
    "        detections['num_qdetections'] = num_detections\n",
    "        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "        label_id_offset = 1\n",
    "\n",
    "        # The bounding boxes of all detected gestures are drawn on top of the original frame\n",
    "        # with their corresponding label.\n",
    "        # max_boxes_to_draw=1 doesn't let two overlapping gestures to be recognized at once\n",
    "        # min_score_thresh=.7 ignores all detections with an accuracy rate lower than 70%\n",
    "        viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                    image_np,\n",
    "                    detections['detection_boxes'],\n",
    "                    detections['detection_classes']+label_id_offset,\n",
    "                    detections['detection_scores'],\n",
    "                    category_index,\n",
    "                    use_normalized_coordinates=True,\n",
    "                    max_boxes_to_draw=1,\n",
    "                    min_score_thresh=.7,\n",
    "                    agnostic_mode=False)\n",
    "\n",
    "    # Process detected gestures\n",
    "    for index, val in enumerate(detections['detection_scores']):\n",
    "        if(val > 0.7):\n",
    "            det = category_index[detections['detection_classes'][index]+label_id_offset]['name']\n",
    "            if (currentLetter[0] == \"\"):\n",
    "                currentLetter = (det, 1)\n",
    "            elif (currentLetter[0] == det):\n",
    "                currentLetter = (det, currentLetter[1]+1)\n",
    "            else:\n",
    "                previousLetter = currentLetter[0]\n",
    "                currentLetter = (det, 1)\n",
    "            \n",
    "            if (previousLetter == specialGestures[\"delete\"][0] and currentLetter[0] == specialGestures[\"delete\"][1]):\n",
    "                buffer = buffer[:-1]\n",
    "                previousLetter = \"\"\n",
    "                currentLetter = (\"\", 0)\n",
    "            elif (previousLetter == specialGestures[\"space\"][0] and currentLetter[0] == specialGestures[\"space\"][1]):\n",
    "                buffer += \" \"\n",
    "                previousLetter = \"\"\n",
    "                currentLetter = (\"\", 0)\n",
    "            elif (previousLetter == specialGestures[\"clear\"][0] and currentLetter[0] == specialGestures[\"clear\"][1]):\n",
    "                buffer = \"\"\n",
    "                previousLetter = \"\"\n",
    "                currentLetter = (\"\", 0)\n",
    "            elif (previousLetter == specialGestures[\"enter\"][0] and currentLetter[0] == specialGestures[\"enter\"][1]):\n",
    "                print(buffer)\n",
    "                buffer = \"\"\n",
    "                previousLetter = \"\"\n",
    "                currentLetter = (\"\", 0)\n",
    "            elif (currentLetter[1] == 5):\n",
    "                buffer += det\n",
    "                previousLetter = \"\"\n",
    "                currentLetter = (\"\", 0)\n",
    "        else:\n",
    "            previousLetter = \"\"\n",
    "            currentLetter = (\"\", 0)\n",
    "        break\n",
    "        \n",
    "    # Septup window text\n",
    "    text.set_text(buffer)\n",
    "\n",
    "    # The output is displayed on an interactive window\n",
    "    image_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n",
    "    if result is None:\n",
    "        plt.axis(\"off\")\n",
    "        result = plt.imshow(image_np)\n",
    "        plt.title(\"Real-time Sign Detection\")\n",
    "        plt.show() \n",
    "    else:\n",
    "        result.set_data(image_np)\n",
    "        gui.canvas.draw()\n",
    "        gui.canvas.flush_events()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
